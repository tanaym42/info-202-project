{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdmluJrygB5S"
      },
      "source": [
        "# Pensieve: Chrome History Exploration Tool\n",
        "\n",
        "This project seeks to develop an application to help users browse through their chrome browser history more efficiently. It uses elastic search and the chrome's already existing history database to serve as the basis of the project. \n",
        "\n",
        "The user will need to load their file manually into the colab notebook environment. **If this is not possible, the project evaluator may use the History sqllite file that I have attached as part of my submission.** This has not been included on github due to privacy concerns. \n",
        "\n",
        "**For Mac users, the file will be found in:**\n",
        "* **~/Library/Application\\ Support/Google/Chrome/Default/History**\n",
        "\n",
        "We may also see 'Profile 1' instead of 'Default'. \n",
        "\n",
        "Follow the steps to reach the file. \n",
        "\n",
        "1. Open the 'Finder' application and click on 'Go' in the upper tool bar. \n",
        "2. Keep the 'options' button on the keyboard pressed to make hidden files visible. You should see a library file become visible. \n",
        "3. In Library, navigate to Application Support, Google, Chrome and Default or Profile 1. \n",
        "4. Copy the 'History' file into the desktop. Save it as 'History'. \n",
        "5. Upload the file into the colab notebook 'files' using the file shaped icon on the left. \n",
        "\n",
        "\n",
        "**For Windows users, the file will be found in one of the following:**\n",
        "* **C:\\Users\\<username>\\AppData\\Local\\Google\\Chrome\\User Data\\Default**\n",
        "* **C:\\Users\\<username>\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cache**\n",
        "\n",
        "\n",
        "Note that the code will only function for chrome browsing history. Other browsers are not currently supported. \n",
        "\n",
        "Note: If you are getting a 'malformed' error, please try again in 10 seconds...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO4_YL7D9mrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811db5ff-51cf-45e2-90af-3dffd8775cdf"
      },
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "import json\n",
        " \n",
        "\n",
        "try:\n",
        "    # Making a connection between sqlite3 database and Python Program\n",
        "    conn = sqlite3.connect(\"History\")\n",
        "\n",
        "    #This is the important part, here we are setting row_factory property of\n",
        "    #connection object to sqlite3.Row(sqlite3.Row is an implementation of\n",
        "    #row_factory)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    \n",
        "    # Creating cursor object using connection object\n",
        "    c = conn.cursor()\n",
        "    \n",
        "    # executing our sql query to load urls\n",
        "    c.execute('SELECT * FROM urls')\n",
        "    urls = [dict(row) for row in c.fetchall()]\n",
        "   \n",
        "    # executing our sql query to load searches\n",
        "    c.execute('SELECT * FROM keyword_search_terms')\n",
        "    searches = [dict(row) for row in c.fetchall()]\n",
        "    \n",
        "    # executing our sql query to load the visits data\n",
        "    c.execute('SELECT * FROM visits')\n",
        "    visits = [dict(row) for row in c.fetchall()]\n",
        " \n",
        "except sqlite3.Error as error:\n",
        "    print(\"Failed to execute the above query\", error)\n",
        "     \n",
        "finally:\n",
        "   \n",
        "    # Inside Finally Block, If connection is\n",
        "    # open, we need to close it\n",
        "    if conn:\n",
        "         \n",
        "        # using close() method, we will close\n",
        "        # the connection\n",
        "        conn.close()\n",
        "         \n",
        "        # After closing connection object, we\n",
        "        # will print \"the sqlite connection is\n",
        "        # closed\"\n",
        "        print(\"The sqlite connection is succesfully closed, you may proceed...\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sqlite connection is succesfully closed, you may proceed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next piece of code will install the relevant dependencies. This has been adapted from the University of Berkeley School of Information Colab Notebook provided for the purpose of carrying out an assignment for this class. The link is provided here: \n",
        "https://colab.research.google.com/drive/1B15lS5j-CkzZf3xXLQS9ORfEYhSLsl7n#scrollTo=DPPCGPqaBb0_"
      ],
      "metadata": {
        "id": "es_FRHuTgES3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2vflNaIf905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031663f9-ef32-41fa-dc2f-8181b19f8049"
      },
      "source": [
        "\n",
        "# This code installs relevant dependencies for this project.\n",
        "# NOTE: this cell will take about 30 seconds to run\n",
        "\n",
        "!pip install elasticsearch==7.17\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.10.1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting elasticsearch==7.17\n",
            "  Downloading elasticsearch-7.17.0-py2.py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elasticsearch==7.17) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from elasticsearch==7.17) (1.24.3)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.17.0\n",
            "--2022-12-14 03:03:53--  https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
            "Resolving artifacts.elastic.co (artifacts.elastic.co)... 34.120.127.130, 2600:1901:0:1d7::\n",
            "Connecting to artifacts.elastic.co (artifacts.elastic.co)|34.120.127.130|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318801277 (304M) [application/x-gzip]\n",
            "Saving to: ‘elasticsearch-7.10.1-linux-x86_64.tar.gz’\n",
            "\n",
            "elasticsearch-7.10. 100%[===================>] 304.03M  28.1MB/s    in 18s     \n",
            "\n",
            "2022-12-14 03:04:11 (16.9 MB/s) - ‘elasticsearch-7.10.1-linux-x86_64.tar.gz’ saved [318801277/318801277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to set up our elastic search client and index for our search. We are creating a 'searches' index in our client and specifying the fields in the data which must be indexed for the purpose of searches and ranking. \n",
        "\n",
        "Apart from the text fields, such as 'url title' and 'google search query' we also specify the date and time which each url was accessed on. These will all be used to search and rank our search results. \n",
        "\n"
      ],
      "metadata": {
        "id": "83n6-qz6pe7T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4agAN4GbTQBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6023fc82-b7af-4f27-879a-2333c35709d3"
      },
      "source": [
        "# Let's set up our ElasticSearch instance on our linux system. \n",
        "# NOTE: this will take ~1 minute to run\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "!sleep 30\n",
        "\n",
        "# NOTE: This code is used to set up the index that will be searched through. \n",
        "# The output should look like: {'acknowledged': True, 'index': 'conversations', 'shards_acknowledged': True}\n",
        "\n",
        "# Let's set up the infrastructure for our elasticsearch index\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "# This code connects to the ElasticSearch instance we started in the previous cell.\n",
        "es = Elasticsearch(hosts=[\"http://localhost:9200\"], timeout=60, retry_on_timeout=True)\n",
        "\n",
        "# We specify that we would like to use BM25 similiarity and specify the fields of the data that we would like to index. \n",
        "es.indices.create(index=\"searches\", settings= {\n",
        "    \"similarity\": {\n",
        "      \"default\": { \n",
        "        \"type\": \"BM25\"\n",
        "       }}},\n",
        "    mappings = {\n",
        "      \n",
        "      \"properties\": {\n",
        "        \"title\":    { \"type\": \"text\",\n",
        "                        \"index\": True},\n",
        "        \"term\":    { \"type\": \"text\",\n",
        "                        \"index\": True},  \n",
        "        \"normalized_term\":   { \"type\": \"text\",\n",
        "                        \"index\": True},\n",
        "         \"fixed_visit_time\": {\n",
        "                    \"type\": \"date\",\n",
        "                    \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
        "                }                    \n",
        "     }})"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'searches_index'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The below function will be used to convert the time values stored in the chrome database\n",
        "# It uses the Webkit time, which measures microseconds from 1st Jan, 1601 (!)\n",
        "\n",
        "import datetime\n",
        "import re\n",
        "def date_from_webkit(webkit_timestamp):\n",
        "    epoch_start = datetime.datetime(1601,1,1)\n",
        "    delta = datetime.timedelta(microseconds=int(webkit_timestamp))\n",
        "    final = epoch_start + delta\n",
        "    return str(final)[:19]\n",
        "\n",
        "# Function to return only the domain name using regex.\n",
        "\n",
        "def return_domain(url_link):\n",
        "  domain_name = re.findall('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+)', url_link)\n",
        "  return domain_name\n"
      ],
      "metadata": {
        "id": "CJscq8OkueBE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1xM0jc43AfV"
      },
      "source": [
        "'''\n",
        "The below are all functions to be used to index the information correctly and display the results. \n",
        "Function based on: https://bit.ly/3wu0S4E and the assignment document. \n",
        "'''\n",
        "\n",
        "# The below function will be used to convert the time values stored in the chrome database\n",
        "# It uses the Webkit time, which measures microseconds from 1st Jan, 1601 (!)\n",
        "\n",
        "import datetime\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def date_from_webkit(webkit_timestamp):\n",
        "    epoch_start = datetime.datetime(1601,1,1)\n",
        "    delta = datetime.timedelta(microseconds=int(webkit_timestamp))\n",
        "    final = epoch_start + delta\n",
        "    return str(final)[:19]\n",
        "\n",
        "# Function to return only the domain name using regex.\n",
        "\n",
        "def return_domain(url_link):\n",
        "  domain_name = re.findall('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+)', url_link)\n",
        "  return domain_name\n",
        "\n",
        "def table(category, query, rows):\n",
        "    html = \"\"\"\n",
        "    <style type='text/css'>\n",
        "    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n",
        "    table {\n",
        "      border-collapse: collapse;\n",
        "      width: 900px;\n",
        "    }\n",
        "    th, td {\n",
        "        border: 1px solid #9e9e9e;\n",
        "        padding: 10px;\n",
        "        font: 15px Oswald;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    html += \"<h3>[%s] %s</h3><table><thead><tr><th>Score</th><th>URL Title</th><th>Domain Name</th><th>Last Visit</th></tr></thead>\" % (category, query)\n",
        "    for score,title,domain,lastvisit in rows:\n",
        "        html += \"<tr><td>%.4f</td><td>%s</td><td>%s</td><td>%s</td></tr>\" % (score, title,domain,lastvisit)\n",
        "    html += \"</table>\"\n",
        "\n",
        "    display(HTML(html))\n",
        "\n",
        "def search(query, limit):\n",
        "  query_input = {\n",
        "            \"query\": {\n",
        "              \"bool\": {\n",
        "                \"must\": [\n",
        "                  {\n",
        "                    \"multi_match\": {\n",
        "                        \"query\" :    query, \n",
        "                        \"fields\": [ \"term^3\", \"title\" ] \n",
        "                    }\n",
        "                  }\n",
        "                ],\n",
        "              \"filter\": \n",
        "                  {\n",
        "                    \"range\": {\n",
        "                      \"fixed_visit_time\": {\n",
        "                        \"gte\": \"2022-01-21 00:00:00\"\n",
        "                      }\n",
        "                    }\n",
        "                  }\n",
        "              \n",
        "              }\n",
        "            }\n",
        "}\n",
        "\n",
        "  results = []\n",
        "  for result in es.search(index=\"searches\", body=query_input)[\"hits\"][\"hits\"]:\n",
        "    source = result[\"_source\"]\n",
        "    results.append((min(result[\"_score\"], 18) / 18, source[\"title\"], source[\"domain\"], source[\"fixed_visit_time\"]))\n",
        "\n",
        "  return results\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The next bit of code prepares our information to be inputted into the index \n",
        "using the buffer function provided by elastic search. The data itself is already\n",
        "in a dictionary so we only modify some fields (tagging, etc) before pushing. \n",
        "'''\n",
        "\n",
        "rows = 0\n",
        "\n",
        "# Creating a copy of urls to input into the index\n",
        "buffer = urls.copy()\n",
        "\n",
        "# These are lists of different tags that may arise for our urls. \n",
        "social_media = ['twitter.com', 'facebook.com', 'instagram.com', 'web.whatsapp.com']\n",
        "music_platforms = ['open.spotify.com','music.apple']\n",
        "movies_tv = ['netflix.com', 'primevideo.com', 'hulu.com', 'showtime.com', 'tv.apple']\n",
        "news_reading = ['cnn', 'guardian', 'newyorkpost']\n",
        "research_reading = ['arxiv', 'jstor', 'elsevier']\n",
        "\n",
        "\n",
        "for url in buffer:\n",
        "\n",
        "  # Here, we add an id and index to the data before pushing it to elastic search. \n",
        "  es_data = {\"_id\": rows, \"_index\": \"searches\"}\n",
        "  url.update(es_data)\n",
        "\n",
        "  for search in searches:\n",
        "    if url[\"id\"] == search[\"url_id\"]:\n",
        "      url.update(search)\n",
        "  \n",
        "  url[\"domain\"] = return_domain(url[\"url\"])\n",
        "\n",
        "  url[\"fixed_visit_time\"] = date_from_webkit(url[\"last_visit_time\"])\n",
        "\n",
        "  rows+=1\n",
        "\n",
        "# Pushing the data into elastic search. \n",
        "helpers.bulk(es, buffer)\n",
        "buffer = []\n",
        "\n",
        "#The number of rows is the number of individual url visits. \n",
        "print(\"Total urls inserted: {}\".format(rows))\n",
        "\n",
        "\n",
        "# The below will confirm the information we have about the index we have created. \n",
        "es.indices.get_mapping(index=\"searches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO8zeZJihs_f",
        "outputId": "8990e7de-559c-4ad9-ccb8-8b25d3d47602"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total urls inserted: 16108\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'searches_index': {'mappings': {'properties': {'domain': {'type': 'text',\n",
              "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
              "    'fixed_visit_time': {'type': 'date', 'format': 'yyyy-MM-dd HH:mm:ss'},\n",
              "    'hidden': {'type': 'long'},\n",
              "    'id': {'type': 'long'},\n",
              "    'keyword_id': {'type': 'long'},\n",
              "    'last_visit_time': {'type': 'long'},\n",
              "    'normalized_term': {'type': 'text'},\n",
              "    'term': {'type': 'text'},\n",
              "    'title': {'type': 'text'},\n",
              "    'typed_count': {'type': 'long'},\n",
              "    'url': {'type': 'text',\n",
              "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
              "    'url_id': {'type': 'long'},\n",
              "    'visit_count': {'type': 'long'}}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This \n",
        "code will run the query on the basis of a user input. \n",
        "'''\n",
        "query_user = input('Enter your query here: ')\n",
        "table(\"Elasticsearch\", query_user, search(query_user, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "65KTvJxh_Zlo",
        "outputId": "68e5ac0b-18d8-4575-d83b-6775d71c2671"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query here: mac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-10cf1a5d9b51>:74: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
            "  for result in es.search(index=\"searches\", body=query_input)[\"hits\"][\"hits\"]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style type='text/css'>\n",
              "    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n",
              "    table {\n",
              "      border-collapse: collapse;\n",
              "      width: 900px;\n",
              "    }\n",
              "    th, td {\n",
              "        border: 1px solid #9e9e9e;\n",
              "        padding: 10px;\n",
              "        font: 15px Oswald;\n",
              "    }\n",
              "    </style>\n",
              "    <h3>[Elasticsearch] mac</h3><table><thead><tr><th>Score</th><th>URL Title</th><th>Domain Name</th><th>Last Visit</th></tr></thead><tr><td>0.8104</td><td>mac miller circles - Google Search</td><td>['google.com']</td><td>2022-09-18 21:09:38</td></tr><tr><td>0.7291</td><td>foxit reader download mac - Google Search</td><td>['google.com']</td><td>2022-08-11 18:51:12</td></tr><tr><td>0.7291</td><td>best epub readers mac - Google Search</td><td>['google.com']</td><td>2022-08-20 05:17:23</td></tr><tr><td>0.7291</td><td>pip not working mac - Google Search</td><td>['google.com']</td><td>2022-08-20 06:06:31</td></tr><tr><td>0.7291</td><td>drawsvg cairo error mac - Google Search</td><td>['google.com']</td><td>2022-08-25 09:01:01</td></tr><tr><td>0.7291</td><td>best epub reader mac - Google Search</td><td>['google.com']</td><td>2022-09-18 17:28:34</td></tr><tr><td>0.7291</td><td>mac miller kurt vonnegut - Google Search</td><td>['google.com']</td><td>2022-09-22 23:14:17</td></tr><tr><td>0.7291</td><td>what is mac \"rsession\" - Google Search</td><td>['google.com']</td><td>2022-11-03 22:15:53</td></tr><tr><td>0.6627</td><td>best free pdf readers mac - Google Search</td><td>['google.com']</td><td>2022-08-11 18:50:45</td></tr><tr><td>0.6627</td><td>splashtop display won't open mac - Google Search</td><td>['google.com']</td><td>2022-08-11 19:17:57</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WM0S3OgG2wK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}